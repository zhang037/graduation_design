{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e654a6f",
   "metadata": {},
   "source": [
    "## 0. 模型叠加预训练 \n",
    "### 0.1 数据处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b16705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料总量的句子数为： 8001\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def remove_quotes(string):\n",
    "    if string.startswith(\"'\") and string.endswith(\"'\"):\n",
    "        string = string.strip(\"'\")\n",
    "    elif string.startswith('\"') and string.endswith('\"'):\n",
    "        string = string.strip('\"')\n",
    "    return string\n",
    "\n",
    "with open('/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/data/original_data/before_translate/biological_strategy.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    # 跳过表头\n",
    "    next(reader)\n",
    "    \n",
    "    # 逐行读取数据, 并将需要叙述存入列表中\n",
    "    text_list = []\n",
    "    \n",
    "    for row in reader:\n",
    "        id = '##id:'+ row[0]\n",
    "        first_fun = row[1]\n",
    "        second_fun = row[2]\n",
    "        third_fun = row[3]\n",
    "        title = row[4]\n",
    "        URL = row[5]\n",
    "        introduction = row[6]\n",
    "        strategy = row[7]\n",
    "        potential = row[8]\n",
    "        related_innovation = row[9]\n",
    "        related_strategy = row[10]\n",
    "        reference = row[11]\n",
    "        \n",
    "        # 删除字符串首尾的‘’符号\n",
    "        introduction = remove_quotes(introduction)\n",
    "        strategy = remove_quotes(strategy).replace(\"', '\",\", \")\n",
    "        potential = remove_quotes(potential).replace(\"', '\",\". \")\n",
    "        \n",
    "        # 将introduction、strategy、potential合并为段落\n",
    "        content = introduction + strategy + potential\n",
    "        \n",
    "        # 按照标点符号划分句子\n",
    "        content_list = []\n",
    "        content_list = content.split('. ')\n",
    "        \n",
    "        \n",
    "        # 将文章编号、标题、introduction、strategy、potential合并，并处理成标注需要的格式\n",
    "        for i in range(len(content_list)):\n",
    "            text = content_list[i] + '.'\n",
    "            text_list.append(text)\n",
    "print(\"语料总量的句子数为：\", len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361f41b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shellfish that live in salt water environments have an easily accessible source of the calcium and carbonate ions needed to build new shells',\n",
       " 'In contrast, those that live in freshwater environments, such as the common pond snail, need to develop clever mechanisms for obtaining those resources since the availability of dissolved calcium and carbonate ions is significantly less than that of their marine cousins.When resources of calcium ions are particularly low, the organism maintains critical calcium requirements for new shell formation by cycling internal sources from previously formed shell',\n",
       " 'Cells create a driving force for the uptake of calcium ions by utilizing the hydrogen ions generated from dissolved carbon dioxide',\n",
       " 'The hydrogen ions essentially exit the cell through a revolving ,  door through which calcium ions summarily enter the cell.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83d46f",
   "metadata": {},
   "source": [
    "### 0.2 模型预训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547bd8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model_bin were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 9.045442857313901e-05\n",
      "Epoch 2/20 - Loss: 0.0001546507264720276\n",
      "Epoch 3/20 - Loss: 3.7101839552633464e-05\n",
      "Epoch 4/20 - Loss: 0.00014673463010694832\n",
      "Epoch 5/20 - Loss: 9.13749317987822e-06\n",
      "Epoch 6/20 - Loss: 5.9845806390512735e-06\n",
      "Epoch 7/20 - Loss: 7.906241080490872e-06\n",
      "Epoch 8/20 - Loss: 2.547132453400991e-06\n",
      "Epoch 9/20 - Loss: 1.2645373317354824e-05\n",
      "Epoch 10/20 - Loss: 4.191711468592985e-06\n",
      "Epoch 11/20 - Loss: 2.033827968261903e-06\n",
      "Epoch 12/20 - Loss: 1.411029188602697e-06\n",
      "Epoch 13/20 - Loss: 8.417607091359969e-07\n",
      "Epoch 14/20 - Loss: 8.101310413621832e-07\n",
      "Epoch 15/20 - Loss: 5.157610303285765e-07\n",
      "Epoch 16/20 - Loss: 6.519995849885163e-07\n",
      "Epoch 17/20 - Loss: 5.230601232142362e-07\n",
      "Epoch 18/20 - Loss: 2.031359372267616e-06\n",
      "Epoch 19/20 - Loss: 2.5495044155832147e-06\n",
      "Epoch 20/20 - Loss: 6.106406544859055e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_bin_pretrain/tokenizer_config.json',\n",
       " './model_bin_pretrain/special_tokens_map.json',\n",
       " './model_bin_pretrain/vocab.txt',\n",
       " './model_bin_pretrain/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 自定义语料库的示例数据\n",
    "corpus = text_list\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, corpus, tokenizer):\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.corpus[index]\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = [self.tokenizer.cls_token_id] + token_ids + [self.tokenizer.sep_token_id]\n",
    "        segment_ids = [0] * len(input_ids)  # 单句子任务，所有token属于同一个segment\n",
    "        return torch.tensor(input_ids), torch.tensor(segment_ids)\n",
    "\n",
    "# 初始化BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join('.', 'model_bin'))\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join('.', 'model_bin'))\n",
    "\n",
    "# 定义自定义数据加载器\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    segment_ids = [item[1] for item in batch]\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    segment_ids = pad_sequence(segment_ids, batch_first=True)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'token_type_ids': segment_ids\n",
    "    }\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = CustomDataset(corpus, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "# 模型训练设置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "total_epochs = 20\n",
    "\n",
    "# 模型训练\n",
    "for epoch in range(total_epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=input_ids  # 使用标签代替masked_lm_labels参数\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} - Loss: {loss.item()}\")\n",
    "\n",
    "# 保存训练好的模型\n",
    "model.save_pretrained(os.path.join('.', 'model_bin_pretrain'))\n",
    "tokenizer.save_pretrained(os.path.join('.', 'model_bin_pretrain'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a677665",
   "metadata": {},
   "source": [
    "## 1. 模型构建与训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f20eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b438f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f84dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def read_data(file):\n",
    "    all_token_list = []\n",
    "    all_label_list = []\n",
    "\n",
    "    token_list = []\n",
    "    label_list = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_data = f.read().split(\"\\n\")\n",
    "    lenth_sentence_list = [] #用于存储句子的长度\n",
    "    for data in all_data:\n",
    "        # 这里进行数据处理时，每一句话结束后都会有一个空行，当判断为空行时，说明结束，这一句话存入列表\n",
    "        # 但是在我重新处理的语料中，根据句号.来对句子进行划分\n",
    "        if data == \". O\":\n",
    "            # 统计一下大多数的句子的长度，太长或者太短的句子直接丢掉，不要保存在语料列表当中\n",
    "            lenth_sentence_list.append(len(token_list))\n",
    "            if len(token_list) > 10 and len(token_list) < 70:\n",
    "                all_token_list.append(token_list)\n",
    "                all_label_list.append(label_list)\n",
    "            token_list = []\n",
    "            label_list = []\n",
    "        else:\n",
    "            # print(data)\n",
    "            token, lable = data.split(\" \")\n",
    "            token_list.append(token)\n",
    "            label_list.append(lable)\n",
    "    return all_token_list, all_label_list, lenth_sentence_list\n",
    "\n",
    "\n",
    "# 以训练集为例构建标签及标签ID映射关系\n",
    "def build_label(train_label):\n",
    "    # PAD用于填充，UNK 出现不认识的label,则给一个UNK的标识\n",
    "    label2idx = {\"PAD\": 0, \"UNK\": 1}\n",
    "    for label_list in train_label:\n",
    "        for lable in label_list:\n",
    "            if lable not in label2idx:\n",
    "                label2idx[lable] = len(label2idx)\n",
    "    return label2idx, list(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ded504",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_F_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train_text, train_label, train_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_S_train.txt\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# test_text, test_label, test_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_S_dev.txt\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# dev_text, dev_label, dev_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_S_dev.txt\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_text, train_label, train_lenth_sentence_list \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_F_train.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m test_text, test_label, test_lenth_sentence_list \u001b[38;5;241m=\u001b[39m read_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_F_dev.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m dev_text, dev_label, dev_lenth_sentence_list \u001b[38;5;241m=\u001b[39m read_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_F_dev.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mread_data\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      6\u001b[0m token_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m label_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     all_data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m lenth_sentence_list \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m#用于存储句子的长度\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_F_train.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# train_text, train_label, train_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_S_train.txt\")\n",
    "# test_text, test_label, test_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_S_dev.txt\")\n",
    "# dev_text, dev_label, dev_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_S_dev.txt\")\n",
    "\n",
    "train_text, train_label, train_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_F_train.txt\")\n",
    "test_text, test_label, test_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_F_dev.txt\")\n",
    "dev_text, dev_label, dev_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/fineturn_data/biological_data/bio_data_F_dev.txt\")\n",
    "\n",
    "\n",
    "# print(test_label, test_text)\n",
    "\n",
    "# 定义标签和对应的数字ID\n",
    "# PAD --> padding\n",
    "# UNK --> unknown\n",
    "# labels_list = ['PAD', 'UNK', 'O', 'B-BFA', 'I-BFA', 'B-BFO', 'I-BFO', 'B-BOP', 'I-BOP', 'B-BEN', 'I-BEN', 'B-BOR', 'I-BOR']\n",
    "# FA 功能行为； FO 功能对象； OP 生物行为；OR 器官；EN 生物体\n",
    "# 将生物器官和生物体合并为生物结构，看看生物行为的数量多不多，不多的话看一下用其他的办法获得吧\n",
    "\n",
    "label2idx, idx2label = build_label(train_label)\n",
    "\n",
    "print(\"label2idx为：\", label2idx)\n",
    "print(\"idx2label为：\", idx2label)\n",
    "print(len(train_lenth_sentence_list))    # 原本有525句子\n",
    "print(len(train_text))       # 符合长度要求的只有254句子\n",
    "print(len(dev_lenth_sentence_list))    # 原本有525句子\n",
    "print(len(dev_text))       # 符合长度要求的只有254句子\n",
    "print(\"train_lenth_sentence_list为：\", train_lenth_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784d05a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 01:11:06.169986: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 01:11:06.298625: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-21 01:11:06.792002: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-21 01:11:06.792051: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-21 01:11:06.792056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from torchcrf import CRF\n",
    "\n",
    "# 调用torch中bert的分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join('.', 'model_bin_pretrain'))\n",
    "\n",
    "# 构建数据集\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, all_text, all_label, label2idx, max_len, tokenizer, is_test=False):\n",
    "        self.all_text = all_text\n",
    "        self.all_label = all_label\n",
    "        self.label2idx = label2idx\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test:\n",
    "            self.max_len = len(self.all_label[index])\n",
    "\n",
    "        # 按批次获得数据\n",
    "        text = self.all_text[index]\n",
    "        # 0到max_len的词有对应的label,后面的用padding（转换为数字张量之后就是0了）补充\n",
    "        label = self.all_label[index][:self.max_len]\n",
    "        \n",
    "        # 这里的encode函数要做的事：\n",
    "        # 对词进行编码，将词转换为在词表中对应的数字\n",
    "        # 对长度大于max_length的语句进行截断（超出了max_length的直接没有了），对长度不足max_length的语句进行padding填充\n",
    "        # 添加特殊的标记符\n",
    "        # 返回tensor类型的张量，即将词进行编码为torch张量\n",
    "        text_index = self.tokenizer.encode(text,\n",
    "                                           add_special_tokens=True,\n",
    "                                           max_length=self.max_len + 2,        # 加上两个左右标识符\n",
    "                                           padding=\"max_length\",\n",
    "                                           truncation=True,\n",
    "                                           return_tensors=\"pt\")        # pt 返回pytorch的张量\n",
    "        label_index = [0] + [self.label2idx.get(l, 1) for l in label] + [0] + [0] * (max_len - len(text))\n",
    "        # 将编码后的label转换为torch张量\n",
    "        label_index = torch.tensor(label_index)\n",
    "        # 统一text_index和label_index的形状和维度\n",
    "        return text_index.reshape(-1), label_index, len(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.all_text.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05db8e91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 训练+测试\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m BertDataset(\u001b[43mtrain_text\u001b[49m,\n\u001b[1;32m     18\u001b[0m                             train_label,\n\u001b[1;32m     19\u001b[0m                             label2idx,\n\u001b[1;32m     20\u001b[0m                             max_len,\n\u001b[1;32m     21\u001b[0m                             tokenizer)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 在处理好的语料中随机获得batch_size个句子，作为一批语料输入模型，shuffle是否要随机获得语料\u001b[39;00m\n\u001b[1;32m     23\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset,\n\u001b[1;32m     24\u001b[0m                               batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     25\u001b[0m                               shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_text' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "max_len = 70\n",
    "\n",
    "# 训练+测试\n",
    "\n",
    "train_dataset = BertDataset(train_text,\n",
    "                            train_label,\n",
    "                            label2idx,\n",
    "                            max_len,\n",
    "                            tokenizer)\n",
    "# 在处理好的语料中随机获得batch_size个句子，作为一批语料输入模型，shuffle是否要随机获得语料\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "dev_dataset = BertDataset(dev_text,\n",
    "                          dev_label,\n",
    "                          label2idx,\n",
    "                          max_len,\n",
    "                          tokenizer)\n",
    "dev_dataloader = DataLoader(dev_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False)\n",
    "\n",
    "print(\"train_dataset的数据类型为： \", type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20273e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from torchcrf import CRF\n",
    "\n",
    "# 构建基于Bert_LSTM_CRF的模型\n",
    "class Bert_LSTM_CRF_NerModel(nn.Module):\n",
    "    def __init__(self, lstm_hidden, class_num, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(os.path.join('.', 'model_bin_pretrain'))\n",
    "        \n",
    "        # 为了省显存做的，如果不想省可以删掉\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # bidirectional 调成True的时候，Linear层的lstm_hidden需要乘2\n",
    "        self.lstm = nn.LSTM(768,\n",
    "                            lstm_hidden,\n",
    "                            batch_first=True,\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # 线性分类器\n",
    "        self.classifier = nn.Linear(lstm_hidden*2, class_num)\n",
    "        \n",
    "        # 条件随机场\n",
    "        self.crf = CRF(class_num, batch_first=True)\n",
    "\n",
    "    # 前馈层\n",
    "    def forward(self, batch_index, batch_label=None):   \n",
    "        # 传了label数据的时候，就是训练，就要看loss,没传label数据的时候，就直接对分类预测的结果pre进行解码\n",
    "        if batch_label is not None:\n",
    "            # 按批次传入数据到bert编码层\n",
    "            bert_out = self.bert(batch_index)\n",
    "            # bert_out0 字符级别的特征信息 bert_out1 篇章级别的特征信息\n",
    "            bert_out0, bert_out1 = bert_out[0], bert_out[1]  \n",
    "            # 命名实体识别是序列标注任务，因此需要bert_out0，字符级别的特征信息\n",
    "            lstm_out, _ = self.lstm(bert_out0)\n",
    "            lstm_out = self.dropout(lstm_out)\n",
    "            # 调用线性分类器完成预测任务\n",
    "            pre = self.classifier(lstm_out)\n",
    "            # loss = self.loss_fun(pre.reshape(-1, pre.shape[-1]),batch_label.reshape(-1))\n",
    "            loss = -self.crf(pre, batch_label)\n",
    "            return loss\n",
    "        else:\n",
    "            # 按批次传入数据到bert编码层\n",
    "            bert_out = self.bert(batch_index)\n",
    "            # bert_out0 字符级别的特征信息 bert_out1 篇章级别的特征信息\n",
    "            bert_out0, bert_out1 = bert_out[0], bert_out[1]  \n",
    "            # 命名实体识别是序列标注任务，因此需要bert_out0，字符级别的特征信息\n",
    "            lstm_out, _ = self.lstm(bert_out0)\n",
    "            # 调用线性分类器完成预测任务\n",
    "            pre = self.classifier(lstm_out)\n",
    "            \n",
    "            pre = self.crf.decode(pre)\n",
    "            return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0954b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from torchcrf import CRF\n",
    "\n",
    "epoch = 200\n",
    "lr = 0.0005\n",
    "lstm_hidden = 128\n",
    "# 根据“训练+验证” “测试” “输入” 三种状态选择True或False\n",
    "do_train = True\n",
    "do_test = False\n",
    "do_input = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_train:\n",
    "        # 加载模型，并将模型放到cuda上\n",
    "        model = Bert_LSTM_CRF_NerModel(lstm_hidden, len(label2idx)).cuda()\n",
    "        \n",
    "        # opt是优化器，这里采用Adam的优化方式\n",
    "        opt = AdamW(model.parameters(), lr)\n",
    "\n",
    "        best_score = -1\n",
    "        for e in range(epoch):\n",
    "            model.train()\n",
    "            batch_index = 1    # 设置该变量记录当前是训练模型的第几批，每一批都有16个句子，所以总共有12批    \n",
    "            for batch_text_index, batch_label_index, batch_len in train_dataloader:\n",
    "                # 按批获得数据，数据放入cuda中\n",
    "                batch_text_index = batch_text_index.cuda()\n",
    "                batch_label_index = batch_label_index.cuda()\n",
    "                # 数据读入读入模型中，获得训练的loss\n",
    "                loss = model.forward(batch_text_index, batch_label_index)\n",
    "                loss.backward()\n",
    "                # opt优化器优化\n",
    "                opt.step()\n",
    "                # 梯度归0\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                print(\"当前的批次为：\", batch_index)\n",
    "                print(f'loss{loss:.2f}')\n",
    "                batch_index = batch_index + 1\n",
    "            \n",
    "            print('-----------------------------------------------')\n",
    "            print('********  have trained ' + str(e+1) + ' epoch!  ********')\n",
    "            print('-----------------------------------------------')\n",
    "            \n",
    "            # 模型训练结束之后，开始进行测试\n",
    "            model.eval()\n",
    "            all_pre = []    # 记录所有的预测label\n",
    "            all_tag = []    # 记录所有的真实label\n",
    "            for batch_text_index, batch_label_index, batch_len in dev_dataloader:\n",
    "                batch_text_index = batch_text_index.cuda()\n",
    "                batch_label_index = batch_label_index.cuda()\n",
    "                # 获得模型预测的结果\n",
    "                pre = model.forward(batch_text_index)\n",
    "                # 加载真实的label\n",
    "                tag = batch_label_index.tolist()\n",
    "\n",
    "                for p, t, l in zip(pre, tag, batch_len):\n",
    "                    p = p[1:1 + l]\n",
    "                    t = t[1:1 + l]\n",
    "\n",
    "                    p = [idx2label[i] for i in p]\n",
    "                    t = [idx2label[i] for i in t]\n",
    "\n",
    "                    all_pre.append(p)\n",
    "                    all_tag.append(t)\n",
    "\n",
    "                # 获得f1_score\n",
    "                f1_score = seqeval_f1_score(all_tag, all_pre)\n",
    "                # 记录最好的f1_score结果，并将得到该结果的模型文件.pt保存下来\n",
    "                if f1_score > best_score:\n",
    "                    torch.save(model, 'best_modelBio_pretrain.pt')\n",
    "                    best_score = f1_score\n",
    "                    \n",
    "                print(f'********  best_score:{best_score}  ********')\n",
    "                print('-----------------------------------------------')\n",
    "                print(f'********  f1_score:{f1_score}  ********')\n",
    "                print('-----------------------------------------------')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20080cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_score)\n",
    "print(len(all_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba0cf63",
   "metadata": {},
   "source": [
    "## 2 使用新的模型预测数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b4b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def read_data(file):\n",
    "    all_token_list = []\n",
    "    all_label_list = []\n",
    "\n",
    "    token_list = []\n",
    "    label_list = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_data = f.read().split(\"\\n\")\n",
    "    lenth_sentence_list = [] #用于存储句子的长度\n",
    "    for data in all_data:\n",
    "        # 但是在我重新处理的语料中，根据句号.来对句子进行划分\n",
    "        if len(data) > 0:\n",
    "            if data == \". O\":\n",
    "                # 统计一下大多数的句子的长度，太长或者太短的句子直接丢掉，不要保存在语料列表当中\n",
    "                if len(token_list) > 0: \n",
    "                    lenth_sentence_list.append(len(token_list))\n",
    "                    all_token_list.append(token_list)\n",
    "                    all_label_list.append(label_list)\n",
    "                token_list = []\n",
    "                label_list = []\n",
    "            else:\n",
    "                token, lable = data.split(\" \")\n",
    "                token_list.append(token)\n",
    "                label_list.append(lable)\n",
    "    return all_token_list, all_label_list, lenth_sentence_list\n",
    "dev_text, dev_label, dev_lenth_sentence_list = read_data(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/original_data/biological/bio_dev_data.txt\")\n",
    "print(\"最长的句子为：\", max(dev_lenth_sentence_list))\n",
    "print(\"有多少条句子：\", len(dev_lenth_sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from torchcrf import CRF\n",
    "\n",
    "# 调用torch中bert的分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join('.', 'model_bin_pretrain'))\n",
    "\n",
    "# 构建数据集\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, all_text, all_label, label2idx, max_len, tokenizer, is_test=False):\n",
    "        self.all_text = all_text\n",
    "        self.all_label = all_label\n",
    "        self.label2idx = label2idx\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test:\n",
    "            self.max_len = len(self.all_label[index])\n",
    "\n",
    "        # 按批次获得数据\n",
    "        text = self.all_text[index]\n",
    "        # 0到max_len的词有对应的label,后面的用padding（转换为数字张量之后就是0了）补充\n",
    "        label = self.all_label[index][:self.max_len]\n",
    "        \n",
    "        # 这里的encode函数要做的事：\n",
    "        # 对词进行编码，将词转换为在词表中对应的数字\n",
    "        # 对长度大于max_length的语句进行截断（超出了max_length的直接没有了），对长度不足max_length的语句进行padding填充\n",
    "        # 添加特殊的标记符\n",
    "        # 返回tensor类型的张量，即将词进行编码为torch张量\n",
    "        text_index = self.tokenizer.encode(text,\n",
    "                                           add_special_tokens=True,\n",
    "                                           max_length=self.max_len + 2,        # 加上两个左右标识符\n",
    "                                           padding=\"max_length\",\n",
    "                                           truncation=True,\n",
    "                                           return_tensors=\"pt\")        # pt 返回pytorch的张量\n",
    "        label_index = [0] + [self.label2idx.get(l, 1) for l in label] + [0] + [0] * (max_len - len(text))\n",
    "        # 将编码后的label转换为torch张量\n",
    "        label_index = torch.tensor(label_index)\n",
    "        # 统一text_index和label_index的形状和维度\n",
    "        return text_index.reshape(-1), label_index, len(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.all_text.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ed9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "max_len = max(dev_lenth_sentence_list)\n",
    "\n",
    "dev_dataset = BertDataset(dev_text,\n",
    "                          dev_label,\n",
    "                          label2idx,\n",
    "                          max_len,\n",
    "                          tokenizer)\n",
    "dev_dataloader = DataLoader(dev_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False)\n",
    "\n",
    "print(\"train_dataset的数据类型为： \", type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_pre = []    # 记录所有的预测label\n",
    "all_tag = []    # 记录所有的真实label\n",
    "batch_index = 1\n",
    "for batch_text_index, batch_label_index, batch_len in dev_dataloader:\n",
    "    batch_text_index = batch_text_index.cuda()\n",
    "    batch_label_index = batch_label_index.cuda()\n",
    "    # 获得模型预测的结果(这里是一个批次的数据)\n",
    "    pre = model.forward(batch_text_index)\n",
    "    # 加载真实的label\n",
    "    tag = batch_label_index.tolist()\n",
    "\n",
    "    for p, t, l in zip(pre, tag, batch_len):\n",
    "        p = p[1:1 + l]\n",
    "        t = t[1:1 + l]\n",
    "\n",
    "        p = [idx2label[i] for i in p]\n",
    "        t = [idx2label[i] for i in t]\n",
    "\n",
    "        all_pre.append(p)\n",
    "        all_tag.append(t)\n",
    "\n",
    "    batch_index = batch_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a97355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"语料的总批次为：\", batch_index)\n",
    "print(\"label的总个数为：\", len(all_pre))\n",
    "print(\"字符的总个数为：\", len(dev_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8e8ee",
   "metadata": {},
   "source": [
    "## 3 将模型预测的结果写入文件 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2label_list = []\n",
    "for i in range(len(dev_text)):\n",
    "    word2label_dic = {}\n",
    "    for j in range(len(dev_text[i])):\n",
    "        word2label_dic[dev_text[i][j]] = all_pre[i][j]\n",
    "    word2label_list.append(word2label_dic)\n",
    "print(\"字符-label对的总个数为：\", len(word2label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3594c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081843f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/result_data/biological_data/1.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for i in range(len(word2label_list)):\n",
    "        for key, value in word2label_list[i].items():\n",
    "            file.write(f\"{key} {value}\\n\")\n",
    "        file.write(\". O\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b0c31",
   "metadata": {},
   "source": [
    "## 4 将预测的结果转换格式（有预处理的格式转换为一篇文章下属的实体及label的格式） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_predict_results(file_path):\n",
    "    \"\"\"\n",
    "    Parse results from predicted file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    id2results = {}\n",
    "    category = \"\"\n",
    "    category_set = set()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            category = line.split(' ')[1].split('-')[1].strip()\n",
    "        except:\n",
    "            pass\n",
    "        if category in ('BFA', 'BFO', 'BOR', 'BEN', 'BOP'):\n",
    "            category_set.add(category)\n",
    "\n",
    "    cur_id = \"\"\n",
    "    extracted_cnt = 0\n",
    "    category = \"\"\n",
    "    for line in lines:\n",
    "        if line.startswith('##id:'):\n",
    "            new_id = line[5:].split(\" \")[0].strip()\n",
    "            if cur_id != new_id:\n",
    "                if extracted_cnt > 0 :\n",
    "                    id2results[cur_id] = result_dict\n",
    "                result_dict = {}\n",
    "                for category in category_set:\n",
    "                    result_dict[category] = set()\n",
    "                cur_id = new_id\n",
    "                extracted_cnt += 1\n",
    "            label_word = ''\n",
    "\n",
    "        else:\n",
    "            label = line.split(' ')[1].strip()\n",
    "            if line.split(' ')[0].startswith('##'):\n",
    "                token = line.split(' ')[0][2:]\n",
    "            else:\n",
    "                token = line.split(' ')[0]\n",
    "            if (label == 'O') or (len(label) == 0):\n",
    "                if len(label_word) != 0:\n",
    "                    result_dict[category].add(label_word)\n",
    "                    category = \"\"\n",
    "                    label_word = \"\"\n",
    "            elif label.startswith('B-'):\n",
    "                if len(label_word) != 0:\n",
    "                    result_dict[category].add(label_word)\n",
    "                    category = \"\"\n",
    "                label_word = token\n",
    "                category = label.split('-')[1].strip()\n",
    "            elif label.startswith('I-'):\n",
    "                if len(category) > 0:\n",
    "                    label_word += \" \" + token\n",
    "    id2results[cur_id] = result_dict\n",
    "    return id2results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf09fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word_dict = parse_predict_results(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/result_data/biological_data/标注后的数据/1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f88ec6",
   "metadata": {},
   "source": [
    "## 5.将最终格式写入csv文件，这里包括了文章和关键词实体，以及文章和关键词实体之间的关系 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e92c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/original_data/biological/id和标题的对应关系.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        id2doc_dict = json.loads(line)\n",
    "print(\"文章的篇数为：\", len(id2doc_dict))\n",
    "\n",
    "# 获得 id2doc_dict中的所有键，也就是所有的文章id\n",
    "id_list = list(id2doc_dict.keys())\n",
    "\n",
    "with open(\"/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/datas/result_data/biological_data/格式转换后的数据/doc_BEN.csv\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"文章,BEN,LABEL\\n\")\n",
    "    for key, value in doc_word_dict.items():\n",
    "        id = key\n",
    "        for i in range(len(id_list)):\n",
    "            if id == id_list[i][5:]:\n",
    "                title = id2doc_dict[id_list[i]]    \n",
    "        try:\n",
    "            # BEN的实体类别是集合\n",
    "            BEN_set = value[\"BEN\"]\n",
    "            for ele in BEN_set:\n",
    "                file.write(title)\n",
    "                file.write(\",\")\n",
    "                file.write(ele)\n",
    "                file.write(\",\")\n",
    "                file.write(\"has_BEN\")\n",
    "                file.write(\"\\n\")\n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0b2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhangxianpeng_env",
   "language": "python",
   "name": "zhangxianpeng_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
