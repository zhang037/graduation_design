{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781b730e",
   "metadata": {},
   "source": [
    "## 1 读原文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f89d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def remove_quotes(string):\n",
    "    if string.startswith(\"'\") and string.endswith(\"'\"):\n",
    "        string = string.strip(\"'\")\n",
    "    elif string.startswith('\"') and string.endswith('\"'):\n",
    "        string = string.strip('\"')\n",
    "    return string\n",
    "\n",
    "with open('/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/data/original_data/before_translate/biological_strategy.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    # 跳过表头\n",
    "    next(reader)\n",
    "    \n",
    "    # 逐行读取数据, 并将需要叙述存入列表中\n",
    "    text_list = []\n",
    "    \n",
    "    for row in reader:\n",
    "        doc_dict = {}\n",
    "        id = '##id:'+ row[0]\n",
    "        first_fun = row[1]\n",
    "        second_fun = row[2]\n",
    "        third_fun = row[3]\n",
    "        title = row[4]\n",
    "        URL = row[5]\n",
    "        introduction = row[6]\n",
    "        strategy = row[7]\n",
    "        potential = row[8]\n",
    "        related_innovation = row[9]\n",
    "        related_strategy = row[10]\n",
    "        reference = row[11]\n",
    "        \n",
    "        # 删除字符串首尾的‘’符号\n",
    "        introduction = remove_quotes(introduction)\n",
    "        strategy = remove_quotes(strategy).replace(\"', '\",\", \")\n",
    "        potential = remove_quotes(potential).replace(\"', '\",\". \")\n",
    "        \n",
    "        # 将文章编号、标题、introduction、strategy、potential合并，并处理成标注需要的格式\n",
    "        if len(introduction) > 3 or len(strategy) > 3 or len(potential) > 3:\n",
    "            doc_dict[\"id\"] = id\n",
    "            doc_dict[\"title\"] = title\n",
    "            doc_dict[\"introduction\"] = introduction\n",
    "            doc_dict[\"strategy\"] = strategy\n",
    "            doc_dict[\"potential\"] = potential\n",
    "            text_list.append(doc_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4de4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(text_list))\n",
    "# print(text_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e509386",
   "metadata": {},
   "source": [
    "## 2. 进行依存句法分析 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e71d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# 加载spaCy的英文模型\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 输入待处理的文本并进行依存句法分析\n",
    "parse_result_list = []\n",
    "for doc in text_list:\n",
    "    parse_result = {}\n",
    "    text = doc[\"title\"] + doc[\"introduction\"] + doc[\"strategy\"] + doc[\"potential\"]\n",
    "    \n",
    "    # 对文章进行清洗\n",
    "    text = text.replace(\"\\\\xa0\", \"\")\n",
    "    \n",
    "    # 对文章进行依存句法分析\n",
    "    result = nlp(text)\n",
    "\n",
    "    # 打印每个词语的依存关系、头部词和词性\n",
    "    parse_result[\"id\"] = doc[\"id\"]\n",
    "    parse_result[\"title\"] = doc[\"title\"]\n",
    "    parse_result[\"result\"] = []\n",
    "    for token in result:\n",
    "        parse_result_dict = {}\n",
    "        parse_result_dict[\"text\"] = token.text\n",
    "        parse_result_dict[\"dep_\"] = token.dep_\n",
    "        parse_result_dict[\"head\"] = token.head.text\n",
    "        parse_result_dict[\"pos_\"] = token.pos_\n",
    "        parse_result[\"result\"].append(parse_result_dict)\n",
    "    parse_result_list.append(parse_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74822f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(parse_result_list))\n",
    "parse_result_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a279af2",
   "metadata": {},
   "source": [
    "## 3.将句法分析的结果存入csv表格 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc598fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义CSV文件路径\n",
    "csv_file = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_result.csv'\n",
    "\n",
    "# 将字典逐行写入CSV文件\n",
    "with open(csv_file, 'w', newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # 写入表头\n",
    "    file.write(\"id,title,text,dep_,head,pos_\\n\")\n",
    "    \n",
    "    # 逐行写入数据\n",
    "    for i in range(len(parse_result_list)):\n",
    "        for j in range(len(parse_result_list[i]['result'])):\n",
    "            file.write(parse_result_list[i][\"id\"])\n",
    "            file.write(\",\")\n",
    "            file.write(parse_result_list[i][\"title\"].replace(\",\", \"-\"))\n",
    "            file.write(\",\")\n",
    "            file.write(parse_result_list[i]['result'][j][\"text\"].replace(\",\", \"-\"))\n",
    "            file.write(\",\")\n",
    "            file.write(parse_result_list[i]['result'][j][\"dep_\"])\n",
    "            file.write(\",\")\n",
    "            file.write(parse_result_list[i]['result'][j][\"head\"].replace(\",\", \"-\"))\n",
    "            file.write(\",\")\n",
    "            file.write(parse_result_list[i]['result'][j][\"pos_\"])\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计句法依存关系的数量和词性的数量\n",
    "import csv\n",
    "\n",
    "file_path = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_result.csv'\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "    dep_list = []\n",
    "    pos_list = []\n",
    "    for row in reader:\n",
    "        dep_list.append(row[\"dep_\"])\n",
    "        pos_list.append(row[\"pos_\"])\n",
    "dep_list = list(set(dep_list))\n",
    "pos_list = list(set(pos_list))\n",
    "print(\"依存关系汇总：\", dep_list)\n",
    "print(\"词性汇总：\", pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca1099",
   "metadata": {},
   "source": [
    "### 依存关系（Dependency Relations）：\n",
    "nsubj: 名词主语    \n",
    "dobj: 直接宾语    \n",
    "iobj: 间接宾语    \n",
    "csubj: 从句主语    \n",
    "cc: 并列关系中的连接词\n",
    "conj: 并列关系中的其他词    \n",
    "advmod: 状语    \n",
    "amod: 修饰形容词    \n",
    "neg: 否定词    \n",
    "det: 冠词、限定词\n",
    "aux: 助动词   \n",
    "auxpass: 被动语态的助动词    \n",
    "prep: 介词    \n",
    "pobj: 介词宾语     \n",
    "nummod: 数量修饰词\n",
    "quantmod: 量词修饰词    \n",
    "compound: 复合词    \n",
    "appos: 同位语    \n",
    "mark: 标记词      \n",
    "advcl: 状语从句        \n",
    "acl: 从句修饰    \n",
    "parataxis: 平行结构    \n",
    "discourse: 跨句关系     \n",
    "root: 根节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81091ff",
   "metadata": {},
   "source": [
    "# 词性（Part-of-Speech, POS）：\n",
    "ADJ: 形容词    ADP: 介词    ADV: 副词    AUX: 助动词    CCONJ: 并列连词\n",
    "DET: 冠词、限定词    INTJ: 感叹词    NOUN: 名词    NUM: 数词    PART: 助词\n",
    "PRON: 代词    PROPN: 专有名词    PUNCT: 标点符号    SCONJ: 从属连词\n",
    "SYM: 符号     VERB: 动词     X: 其他\n",
    "\n",
    "#### 一般名词和专有名词是结构，或者功能对象\n",
    "#### 一般动词是行为\n",
    "#### 科学效应一般也都是动词和名词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a9720",
   "metadata": {},
   "source": [
    "## 4. 获得功能知识的句法分析结果 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f2ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file_path = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_result.csv'\n",
    "output_file_path = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_result_F.csv'\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, open(output_file_path, 'w', newline='', encoding=\"utf-8\") as output_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    writer = csv.writer(output_file)\n",
    "    output_file.write(\"id,title,text,dep_,head,pos_\\n\")\n",
    "    \n",
    "    for row in reader:\n",
    "        if row[3] == 'root' or row[3] == 'dobj' or row[3] == 'nsubjpass':\n",
    "            if row[5] == 'NOUN' or row[5] == 'PROPN' or row[5] == 'VERB' or row[5] =='ADJ':\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c6c71",
   "metadata": {},
   "source": [
    "## 5. 获得结构知识的句法分析结果 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebacbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file_path = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_result.csv'\n",
    "output_file_path = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_result_S.csv'\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, open(output_file_path, 'w', newline='', encoding=\"utf-8\") as output_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    writer = csv.writer(output_file)\n",
    "    output_file.write(\"id,title,text,dep_,head,pos_\\n\")\n",
    "    \n",
    "    for row in reader:\n",
    "        if row[3] == 'amod' or row[3] == 'nsubj' or row[3] == 'agent':\n",
    "            if row[5] == 'NOUN' or row[5] == 'PROPN' or row[5] == 'VERB' or row[5] =='ADJ':\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fae3d",
   "metadata": {},
   "source": [
    "## 6.功能结构知识的过滤 \n",
    "### 6.1获得主题相关词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b794c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def remove_quotes(string):\n",
    "    if string.startswith(\"'\") and string.endswith(\"'\"):\n",
    "        string = string.strip(\"'\")\n",
    "    elif string.startswith('\"') and string.endswith('\"'):\n",
    "        string = string.strip('\"')\n",
    "    return string\n",
    "\n",
    "with open('/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/data/original_data/before_translate/biological_strategy.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    # 跳过表头\n",
    "    next(reader)\n",
    "    \n",
    "    # 逐行读取数据, 并将需要叙述存入列表中\n",
    "    text_list = []\n",
    "    \n",
    "    for row in reader:\n",
    "        doc_dict = {}\n",
    "        id = '##id:'+ row[0]\n",
    "        first_fun = row[1]\n",
    "        second_fun = row[2]\n",
    "        third_fun = row[3]\n",
    "        title = row[4]\n",
    "        URL = row[5]\n",
    "        introduction = row[6]\n",
    "        strategy = row[7]\n",
    "        potential = row[8]\n",
    "        related_innovation = row[9]\n",
    "        related_strategy = row[10]\n",
    "        reference = row[11]\n",
    "        \n",
    "        # 删除字符串首尾的‘’符号\n",
    "        introduction = remove_quotes(introduction)\n",
    "        strategy = remove_quotes(strategy).replace(\"', '\",\", \")\n",
    "        potential = remove_quotes(potential).replace(\"', '\",\". \")\n",
    "        \n",
    "        # 将文章编号、标题、introduction、strategy、potential合并，并处理成标注需要的格式\n",
    "        if len(introduction) > 3 or len(strategy) > 3 or len(potential) > 3:\n",
    "            doc_dict[\"id\"] = id\n",
    "            doc_dict[\"title\"] = title\n",
    "            doc_dict[\"introduction\"] = introduction\n",
    "            doc_dict[\"strategy\"] = strategy\n",
    "            doc_dict[\"potential\"] = potential\n",
    "            text_list.append(doc_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import json\n",
    "\n",
    "def get_top_keywords(text, k = 20):\n",
    "    # 创建TF-IDF向量化器，并设置停用词\n",
    "    vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "\n",
    "    # 对文本进行向量化，并计算TF-IDF\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "\n",
    "    # 获取特征词列表\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # 构建关键词和对应的TF-IDF值的字典\n",
    "    keywords = {}\n",
    "    for col in tfidf_matrix.nonzero()[1]:\n",
    "        keywords[feature_names[col]] = tfidf_matrix[0, col]\n",
    "\n",
    "    # 根据TF-IDF值进行排序，并返回前K个关键词\n",
    "    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_keywords = sorted_keywords[:k]\n",
    "\n",
    "    return top_keywords\n",
    "\n",
    "def write_json(doc_tfidf, output_file):\n",
    "    # 数据的写入\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(doc_tfidf, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd22d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_result = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/tfidf_result.jsonl'\n",
    "\n",
    "for doc in text_list:\n",
    "    parse_result = {}\n",
    "    text = doc[\"title\"] + doc[\"introduction\"] + doc[\"strategy\"] + doc[\"potential\"]\n",
    "    \n",
    "    # 对文章进行清洗\n",
    "    text = text.replace(\"\\\\xa0\", \"\")\n",
    "    \n",
    "    # 获取关键词及其TF-IDF值\n",
    "    doc_tfidf = {}\n",
    "    doc_tfidf[\"id\"] = doc[\"id\"]\n",
    "    doc_tfidf[\"title\"] = doc[\"title\"]\n",
    "    doc_tfidf[\"keywords\"] = get_top_keywords(text)\n",
    "    write_json(doc_tfidf,tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ac5dc",
   "metadata": {},
   "source": [
    "### 6.2根据主题相关词汇筛选功能结构知识\n",
    "#### 6.2.1筛选功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49da77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "teidf_result_list = []\n",
    "tfidf_result = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/tfidf_result.jsonl'\n",
    "with open(tfidf_result, 'r') as file:\n",
    "    for line in file:\n",
    "        line_dict = json.loads(line)\n",
    "        doc = {}\n",
    "        doc[line_dict['id']] = line_dict['keywords']\n",
    "        teidf_result_list.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f924af72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##id:0': [['fungi', 0.43515327467107956],\n",
       "  ['molecules', 0.2769157202452324],\n",
       "  ['enzymes', 0.23735633163877065],\n",
       "  ['use', 0.23735633163877065],\n",
       "  ['break', 0.19779694303230888],\n",
       "  ['chemicals', 0.19779694303230888],\n",
       "  ['energy', 0.1582375544258471],\n",
       "  ['pollutants', 0.11867816581938533],\n",
       "  ['chemical', 0.11867816581938533],\n",
       "  ['living', 0.11867816581938533],\n",
       "  ['environment', 0.11867816581938533],\n",
       "  ['natural', 0.11867816581938533],\n",
       "  ['make', 0.11867816581938533],\n",
       "  ['organisms', 0.11867816581938533],\n",
       "  ['certain', 0.07911877721292356],\n",
       "  ['pollution', 0.07911877721292356],\n",
       "  ['process', 0.07911877721292356],\n",
       "  ['waste', 0.07911877721292356],\n",
       "  ['water', 0.07911877721292356],\n",
       "  ['substances', 0.07911877721292356]]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teidf_result_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9dddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "input_file_path = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_result_F.csv'\n",
    "output_file_path = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_tfidf_F.csv'\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, open(output_file_path, 'w', newline='', encoding=\"utf-8\") as output_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    writer = csv.writer(output_file)\n",
    "    output_file.write(\"id,title,text,dep_,head,pos_\\n\")\n",
    "\n",
    "    for row in reader:\n",
    "        for i in range(len(teidf_result_list)):\n",
    "            for key, value in teidf_result_list[i].items():\n",
    "                # 找到对应的文章\n",
    "                if row[0] == key:\n",
    "                    for j in range(len(value)):\n",
    "                        keyword = value[j][0].strip('\"')\n",
    "                        if row[2] == value[j][0] or row[4] == value[j][0]:\n",
    "                            writer.writerow(row)\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159c556",
   "metadata": {},
   "source": [
    "#### 6.2.2筛选结构 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95390520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file_path = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_result_S.csv'\n",
    "output_file_path = '/ssd01/Codes/PersonalCodes/ZhangXianpeng/graduation_design/biological_ner/dependency_parsing_data/parsing_tfidf_S.csv'\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, open(output_file_path, 'w', newline='', encoding=\"utf-8\") as output_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    writer = csv.writer(output_file)\n",
    "    output_file.write(\"id,title,text,dep_,head,pos_\\n\")\n",
    "    \n",
    "    for row in reader:\n",
    "        for i in range(len(teidf_result_list)):\n",
    "            for key, value in teidf_result_list[i].items():\n",
    "                # 找到对应的文章\n",
    "                if row[0] == key:\n",
    "                    for j in range(len(value)):\n",
    "                        keyword = value[j][0].strip('\"')\n",
    "                        if row[2] == value[j][0] or row[4] == value[j][0]:\n",
    "                            writer.writerow(row)\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb750e0",
   "metadata": {},
   "source": [
    "## 7. 获得结构-功能知识对 \n",
    "通过关联nsubj（名词主语）和agent（被动语态中的主语）两种依赖关系来实现\n",
    "\n",
    "（1）如果结构知识中的结构词作为主语出现在功能知识中，那么这个结构知识和功能知识就可以形成一个结构-功能知识对\n",
    "\n",
    "（2）如果结构知识和功能知识中具有相同的动词，那么也可以形成一个结构-功能知识对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44cf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a391331d",
   "metadata": {},
   "source": [
    "## 8.功能结构知识对的存储 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81b20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhangxianpeng_env",
   "language": "python",
   "name": "zhangxianpeng_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
